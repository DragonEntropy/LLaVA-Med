{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llava.model.builder import load_pretrained_model\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from urllib.request import urlopen\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Up to 10 minute load time\n",
    "cwd = os.path.dirname(os.getcwd())\n",
    "print(cwd)\n",
    "model_path = os.path.join(cwd, \"llava-med-v1.5-mistral-7b\")\n",
    "data_path = os.path.join(cwd, \"llava_med_eval_qa50_fig_captions.json\")\n",
    "\n",
    "tokenizer, model, image_processor, context_len = load_pretrained_model(\n",
    "    model_path=model_path,\n",
    "    model_base=None,\n",
    "    model_name='llava-med-v1.5-mistral-7b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template = 'this is a photo of '\n",
    "labels = [\n",
    "    'adenocarcinoma histopathology',\n",
    "    'brain MRI',\n",
    "    'covid line chart',\n",
    "    'squamous cell carcinoma histopathology',\n",
    "    'immunohistochemistry histopathology',\n",
    "    'bone X-ray',\n",
    "    'chest X-ray',\n",
    "    'pie chart',\n",
    "    'hematoxylin and eosin histopathology'\n",
    "]\n",
    "dataset_url = 'https://huggingface.co/microsoft/BiomedCLIP-PubMedBERT_256-vit_base_patch16_224/resolve/main/example_data/biomed_image_classification_example_data/'\n",
    "test_imgs = [\n",
    "    'squamous_cell_carcinoma_histopathology.jpeg',\n",
    "    'H_and_E_histopathology.jpg',\n",
    "    'bone_X-ray.jpg',\n",
    "    'adenocarcinoma_histopathology.jpg',\n",
    "    'covid_line_chart.png',\n",
    "    'IHC_histopathology.jpg',\n",
    "    'chest_X-ray.jpg',\n",
    "    'brain_MRI.jpg',\n",
    "    'pie_chart.png'\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\"},\n",
    "            {\"type\": \"text\", \"text\": \"What do we see in this image?\"},\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image_processor(Image.open(urlopen(dataset_url + img)), do_normalize=False)[\"pixel_values\"] for img in test_imgs]\n",
    "images = (255 * torch.tensor(images).flatten(0, 1)).to(dtype=torch.long).to(device)\n",
    "print(images.shape)\n",
    "print(images)\n",
    "\n",
    "print([template + l for l in labels])\n",
    "texts = tokenizer([template + l for l in labels], padding=True, truncation=True)\n",
    "texts = torch.tensor(texts[\"input_ids\"]).to(dtype=torch.long).to(device)\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug\n",
    "print(f\"texts dtype: {texts.dtype}\")\n",
    "print(f\"images dtype: {images.dtype}\")\n",
    "print(f\"Images batch size: {images.shape[0]}\")\n",
    "print(f\"Texts batch size: {texts.shape[0]}\")  # Should match images batch size\n",
    "\n",
    "print(f\"Model input types: images={images.dtype}, texts={texts.dtype}\")\n",
    "print(f\"Images shape: {images.shape}, Texts shape: {texts.shape}\")\n",
    "print(f\"Images max value: {images.max().item()}\")\n",
    "print(f\"Images min value: {images.min().item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(texts.shape, images.shape)\n",
    "\n",
    "\"\"\"\n",
    "# For doing one entry at a time (reindent the below section)\n",
    "\n",
    "for i in range(texts.shape[0]):\n",
    "    text = texts[i]\n",
    "    image = images[i]\n",
    "    print(text.shape, image.shape)\n",
    "\"\"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    image_features, text_features, logit_scale = model(images, texts)\n",
    "\n",
    "    logits = (logit_scale * image_features @ text_features.t()).detach().softmax(dim=-1)\n",
    "    sorted_indices = torch.argsort(logits, dim=-1, descending=True)\n",
    "\n",
    "    logits = logits.cpu().numpy()\n",
    "    sorted_indices = sorted_indices.cpu().numpy()\n",
    "\n",
    "top_k = -1\n",
    "\n",
    "for i, img in enumerate(test_imgs):\n",
    "    pred = labels[sorted_indices[i][0]]\n",
    "\n",
    "    top_k = len(labels) if top_k == -1 else top_k\n",
    "    print(img.split('/')[-1] + ':')\n",
    "    for j in range(top_k):\n",
    "        jth_index = sorted_indices[i][j]\n",
    "        print(f'{labels[jth_index]}: {logits[i][jth_index]}')\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
